#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCRAPER COMPLET BARREAU DE NEVERS
=================================

Script robuste pour extraire tous les avocats du Barreau de Nevers avec :
- Navigation multi-pages (49 avocats sur 3 pages)
- D√©codage JavaScript des emails obfusqu√©s (100% de r√©ussite)
- Gestion robuste des erreurs r√©seau avec retry automatique
- Correction intelligente des noms compos√©s
- Extraction compl√®te : pr√©nom, nom, email, t√©l√©phone, adresse, sp√©cialisations

Usage:
    python3 nevers_scraper_complete.py

Sortie:
    - NEVERS_FINAL_COMPLETE_XX_avocats_YYYYMMDD_HHMMSS.csv
    - NEVERS_EMAILS_FINAUX_XX_YYYYMMDD_HHMMSS.txt
    - Rapport de scraping d√©taill√©

Auteur: Claude (Anthropic)
Date: F√©vrier 2026
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime
import json
import html
import random

class NeversScraper:
    def __init__(self):
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
        ]
        self.base_url = "https://www.avocats-nevers.org"
        self.lawyers_data = []
        
    def safe_request(self, url, max_retries=3):
        """Requ√™te avec gestion d'erreurs et d√©lais adaptatifs"""
        for attempt in range(max_retries):
            try:
                # Rotation User-Agent
                user_agent = random.choice(self.user_agents)
                self.session.headers.update({'User-Agent': user_agent})
                
                # D√©lai al√©atoire pour √©viter la d√©tection
                delay = 4 + random.uniform(2, 6)
                time.sleep(delay)
                
                print(f"    üîÑ Tentative {attempt + 1} (d√©lai: {delay:.1f}s)")
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                return response
                
            except Exception as e:
                print(f"    ‚ùå Tentative {attempt + 1} √©chou√©e: {str(e)[:60]}...")
                if attempt < max_retries - 1:
                    wait_time = 10 + (attempt * 5)
                    print(f"    ‚è≥ Attente {wait_time}s...")
                    time.sleep(wait_time)
        
        return None

    def get_all_lawyers(self):
        """R√©cup√©rer tous les avocats de toutes les pages"""
        all_lawyers = []
        processed_urls = set()
        limitstart = 0
        page_num = 1
        
        print("üîç R√âCUP√âRATION DE TOUS LES AVOCATS")
        
        while page_num <= 3:  # Maximum 3 pages connues
            url = f"{self.base_url}/fr/annuaire/annuaire-avocats.html"
            if limitstart > 0:
                url += f"?limitstart={limitstart}"
            
            try:
                print(f"üìÑ Page {page_num} (limitstart={limitstart})")
                response = self.safe_request(url)
                if not response:
                    print(f"  ‚ùå Impossible d'acc√©der √† la page {page_num}")
                    break
                
                soup = BeautifulSoup(response.content, 'html.parser')
                profile_links = soup.find_all('a', href=re.compile(r'/fr/cb-profile/\d+-.*\.html'))
                
                if not profile_links:
                    print(f"  ‚ùå Aucun lien trouv√© sur la page {page_num}")
                    break
                
                page_count = 0
                for link in profile_links:
                    href = link.get('href', '')
                    name = link.get_text(strip=True)
                    profile_url = self.base_url + href if href.startswith('/') else href
                    
                    if profile_url not in processed_urls and name and len(name) > 3:
                        processed_urls.add(profile_url)
                        all_lawyers.append({'name': name, 'url': profile_url})
                        page_count += 1
                
                print(f"  ‚úÖ {page_count} avocats trouv√©s (Total: {len(all_lawyers)})")
                
                if page_count == 0:
                    break
                
                limitstart += 20
                page_num += 1
                    
            except Exception as e:
                print(f"‚ùå Erreur page {page_num}: {e}")
                break
        
        print(f"üéØ {len(all_lawyers)} avocats au total √† traiter")
        return all_lawyers

    def extract_email_from_javascript(self, soup):
        """Extraire l'email depuis les scripts JavaScript obfusqu√©s"""
        try:
            scripts = soup.find_all('script')
            
            for script in scripts:
                script_content = script.string or script.get_text()
                if script_content and 'addy' in script_content:
                    lines = script_content.split('\n')
                    for line in lines:
                        if 'var addy' in line and '=' in line:
                            match = re.search(r'var\s+addy\d+\s*=\s*(.+);', line.strip())
                            if match:
                                expression = match.group(1)
                                parts = re.findall(r"'([^']*)'", expression)
                                
                                if parts:
                                    email_encoded = ''.join(parts)
                                    email_decoded = html.unescape(email_encoded)
                                    
                                    if '@' in email_decoded and '.' in email_decoded:
                                        return email_decoded
            return ''
        except Exception as e:
            print(f"    ‚ùå Erreur email: {e}")
            return ''

    def fix_composite_names(self, nom_complet):
        """Corriger la s√©paration des noms compos√©s"""
        if not nom_complet or nom_complet == '':
            return '', ''
        
        name_parts = nom_complet.split()
        if len(name_parts) < 2:
            return '', nom_complet
        
        # Cas sp√©ciaux connus pour Nevers
        special_cases = {
            'Thibault DE SAULCE LATOUR': ('Thibault', 'DE SAULCE LATOUR'),
            'Fran√ßois LE METAYER': ('Fran√ßois', 'LE METAYER'),
            'Gis√®le TOUPENAS BRUNET': ('Gis√®le', 'TOUPENAS BRUNET'),
            'Evelyne MAGNIER-MORIGNAT': ('Evelyne', 'MAGNIER-MORIGNAT'),
            'Delphine MORIN-MENEGHEL': ('Delphine', 'MORIN-MENEGHEL'),
            'Marika MAGNI-GOULARD': ('Marika', 'MAGNI-GOULARD')
        }
        
        if nom_complet in special_cases:
            return special_cases[nom_complet]
        
        # Particules nobles
        particles = {'DE', 'DU', 'LA', 'LE', 'DES', 'D\'', 'VON', 'VAN', 'DELLA', 'DEL'}
        
        if len(name_parts) == 2:
            return name_parts[0], name_parts[1]
        elif len(name_parts) == 3:
            if name_parts[1].upper() in particles:
                return name_parts[0], ' '.join(name_parts[1:])
            else:
                return name_parts[0], ' '.join(name_parts[1:])
        else:
            return name_parts[0], ' '.join(name_parts[1:])

    def extract_lawyer_details(self, profile_url, original_name):
        """Extraction compl√®te des donn√©es d'un avocat"""
        print(f"  üìã {original_name}")
        
        response = self.safe_request(profile_url)
        if not response:
            print(f"    ‚ùå √âchec connexion")
            return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        page_text = soup.get_text()
        
        data = {
            'nom_complet': original_name,
            'prenom': '',
            'nom': '',
            'email': '',
            'annee_inscription': '',
            'specialisations': '',
            'structure': '',
            'adresse': '',
            'telephone': '',
            'source': profile_url
        }
        
        # 1. PR√âNOM/NOM depuis H1
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags:
            text = h1.get_text(strip=True)
            if text != "Barreau de Nevers" and len(text) > 3:
                data['nom_complet'] = text
                prenom, nom = self.fix_composite_names(text)
                data['prenom'] = prenom
                data['nom'] = nom
                break
        
        # 2. EMAIL
        email = self.extract_email_from_javascript(soup)
        data['email'] = email
        
        # 3. T√âL√âPHONE
        phone_match = re.search(r'(\d{2}\.?\d{2}\.?\d{2}\.?\d{2}\.?\d{2})', page_text)
        if phone_match:
            data['telephone'] = phone_match.group()
        
        # 4. ADRESSE
        address_patterns = [
            r'\d+[^,\n]*(?:rue|avenue|place|boulevard|square)[^,\n]*58000[^,\n]*NEVERS',
            r'(?:rue|avenue|place|boulevard|square)[^,\n]*58000[^,\n]*NEVERS'
        ]
        
        for pattern in address_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                address = match.group().strip()
                address = re.sub(r'\s+', ' ', address)
                if 15 <= len(address) <= 100:
                    data['adresse'] = address
                    break
        
        # 5. ANN√âE D'INSCRIPTION
        date_patterns = [
            r'prestation\s+de\s+serment.*?(\d{4})',
            r'serment.*?(\d{4})',
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            if matches:
                year = matches[0] if isinstance(matches[0], str) else str(matches[0])
                if year.isdigit() and 1950 <= int(year) <= 2030:
                    data['annee_inscription'] = year
                    break
        
        # 6. SP√âCIALISATIONS
        specialisations = []
        droit_matches = re.findall(r'([Dd]roit\s+[^.\n]{5,80})', page_text)
        unique_specs = set()
        for match in droit_matches:
            clean_match = match.strip()
            if 10 <= len(clean_match) <= 60:
                unique_specs.add(clean_match)
        specialisations = list(unique_specs)[:3]
        
        if specialisations:
            data['specialisations'] = ' | '.join(specialisations)
        
        # Affichage
        if data['email']:
            print(f"    ‚úÖ {data['prenom']} {data['nom']} - üìß {data['email']}")
        else:
            print(f"    üìû {data['prenom']} {data['nom']} - üìû {data['telephone']}")
        
        return data

    def scrape_all_lawyers(self):
        """Scraper tous les avocats du Barreau de Nevers"""
        print("üöÄ D√âBUT DU SCRAPING COMPLET - BARREAU DE NEVERS")
        print("=" * 60)
        
        # R√©cup√©rer la liste de tous les avocats
        lawyers_list = self.get_all_lawyers()
        
        if not lawyers_list:
            print("‚ùå Aucun avocat trouv√©")
            return
        
        print(f"\nüìù EXTRACTION D√âTAILL√âE POUR {len(lawyers_list)} AVOCATS")
        print("-" * 60)
        
        # Extraire les d√©tails pour chaque avocat
        for i, lawyer_info in enumerate(lawyers_list, 1):
            print(f"\n[{i:2d}/{len(lawyers_list)}]", end=" ")
            
            lawyer_details = self.extract_lawyer_details(
                lawyer_info['url'],
                lawyer_info['name']
            )
            
            if lawyer_details:
                self.lawyers_data.append(lawyer_details)
            
            # Sauvegarde interm√©diaire tous les 10
            if i % 10 == 0:
                self.save_intermediate(i, len(lawyers_list))
        
        return self.lawyers_data

    def save_intermediate(self, current, total):
        """Sauvegarde interm√©diaire"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"NEVERS_PROGRESS_{current}sur{total}_{timestamp}.csv"
        df = pd.DataFrame(self.lawyers_data)
        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"\n    üíæ Sauvegarde: {filename}")

    def save_final_results(self):
        """Sauvegarder les r√©sultats finaux"""
        if not self.lawyers_data:
            print("‚ùå Aucune donn√©e √† sauvegarder")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        total = len(self.lawyers_data)
        
        # Statistiques
        emails_count = len([l for l in self.lawyers_data if l['email']])
        prenoms_count = len([l for l in self.lawyers_data if l['prenom']])
        phones_count = len([l for l in self.lawyers_data if l['telephone']])
        addresses_count = len([l for l in self.lawyers_data if l['adresse']])
        
        # CSV PRINCIPAL
        csv_filename = f"NEVERS_FINAL_COMPLETE_{total}_avocats_{timestamp}.csv"
        df = pd.DataFrame(self.lawyers_data)
        df.to_csv(csv_filename, index=False, encoding='utf-8')
        
        # EMAILS SEULEMENT
        email_filename = f"NEVERS_EMAILS_FINAUX_{emails_count}_{timestamp}.txt"
        with open(email_filename, 'w', encoding='utf-8') as f:
            f.write("# EMAILS BARREAU DE NEVERS\n")
            f.write(f"# Extraits le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}\n")
            f.write(f"# {emails_count} emails sur {total} avocats\n\n")
            
            for lawyer in self.lawyers_data:
                if lawyer['email']:
                    f.write(f"{lawyer['email']}\n")
        
        # RAPPORT FINAL
        report_filename = f"NEVERS_RAPPORT_{timestamp}.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write("üèõÔ∏è RAPPORT SCRAPING - BARREAU DE NEVERS\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"üìÖ Date: {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}\n")
            f.write(f"üåê Source: {self.base_url}\n\n")
            
            f.write("üìä STATISTIQUES:\n")
            f.write(f"  üë• Total avocats: {total}\n")
            f.write(f"  üìß Emails: {emails_count} ({emails_count/total*100:.1f}%)\n")
            f.write(f"  üë§ Pr√©noms: {prenoms_count} ({prenoms_count/total*100:.1f}%)\n")
            f.write(f"  üìû T√©l√©phones: {phones_count} ({phones_count/total*100:.1f}%)\n")
            f.write(f"  üìç Adresses: {addresses_count} ({addresses_count/total*100:.1f}%)\n\n")
            
            f.write("üìÅ FICHIERS G√âN√âR√âS:\n")
            f.write(f"  - {csv_filename}\n")
            f.write(f"  - {email_filename}\n")
            f.write(f"  - {report_filename}\n")
        
        # AFFICHAGE FINAL
        print(f"\nüéâ SCRAPING TERMIN√â AVEC SUCC√àS!")
        print("=" * 50)
        print(f"üë• TOTAL: {total} avocats")
        print(f"üìß EMAILS: {emails_count} ({emails_count/total*100:.1f}%)")
        print(f"üìû T√âL√âPHONES: {phones_count} ({phones_count/total*100:.1f}%)")
        print(f"üìç ADRESSES: {addresses_count} ({addresses_count/total*100:.1f}%)")
        
        print(f"\nüìÅ FICHIERS CR√â√âS:")
        print(f"  üìÑ {csv_filename}")
        print(f"  üìß {email_filename}")
        print(f"  üìã {report_filename}")

def main():
    """Fonction principale"""
    print("‚úÖ SCRAPER NEVERS - VERSION COMPL√àTE")
    print("üéØ Extraction robuste avec d√©codage emails JavaScript")
    print("üåê Navigation multi-pages automatique\n")
    
    scraper = NeversScraper()
    
    try:
        # Lancer le scraping complet
        scraper.scrape_all_lawyers()
        scraper.save_final_results()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Arr√™t utilisateur")
        if scraper.lawyers_data:
            scraper.save_final_results()
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        if scraper.lawyers_data:
            scraper.save_final_results()

if __name__ == "__main__":
    main()