#!/usr/bin/env python3
"""
Scraper COMPLET pour le Barreau de Chalon-sur-Sa√¥ne
R√©cup√©ration de TOUS les avocats avec navigation entre pages
"""

import time
import json
import csv
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging
import re
import os

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ChalonsurSaoneBarScraperProduction:
    def __init__(self, headless=True):
        self.base_url = "https://www.avocats-chalonsursaone.com/annuaire-des-avocats-chalon-sur-saone/"
        self.lawyers_data = []
        self.headless = headless
        self.setup_driver()
    
    def setup_driver(self):
        """Configuration du driver Chrome"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-gpu")
        
        # Options pour √©viter la d√©tection et optimiser les performances
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")  # Acc√©l√©rer le chargement
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User agent r√©aliste
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.wait = WebDriverWait(self.driver, 15)
    
    def get_pagination_info(self):
        """R√©cup√©rer les informations de pagination"""
        try:
            # Chercher les liens de pagination
            pagination_links = self.driver.find_elements(By.CSS_SELECTOR, "a.pagenav")
            
            page_numbers = []
            for link in pagination_links:
                text = link.text.strip()
                if text.isdigit():
                    page_numbers.append(int(text))
            
            max_page = max(page_numbers) if page_numbers else 1
            
            # V√©rifier aussi dans le texte de la page
            try:
                page_text = self.driver.find_element(By.TAG_NAME, "body").text
                # Chercher des patterns comme "Page 1 sur 7" ou "100 total"
                total_matches = re.findall(r'(\d+)\s*(?:total|membres)', page_text, re.IGNORECASE)
                if total_matches:
                    total_lawyers = int(total_matches[-1])
                    logger.info(f"Total d'avocats d√©tect√©: {total_lawyers}")
            except:
                pass
            
            logger.info(f"Nombre de pages d√©tect√©es: {max_page}")
            return max_page
            
        except Exception as e:
            logger.warning(f"Erreur lors de la d√©tection de pagination: {e}")
            return 7  # Valeur par d√©faut bas√©e sur l'analyse pr√©c√©dente
    
    def get_lawyers_from_current_page(self):
        """R√©cup√©rer tous les avocats de la page actuelle"""
        try:
            # Attendre que le tableau soit charg√©
            table = self.wait.until(EC.presence_of_element_located((By.ID, "cbUserTable")))
            
            # Trouver toutes les lignes d'avocats dans le tbody
            lawyer_rows = self.driver.find_elements(By.CSS_SELECTOR, "#cbUserTable tbody tr")
            
            lawyers_info = []
            
            for i, row in enumerate(lawyer_rows):
                try:
                    # Extraire nom et pr√©nom depuis les liens
                    lastname_link = row.find_element(By.CSS_SELECTOR, ".cbUserListFC_lastname a")
                    firstname_link = row.find_element(By.CSS_SELECTOR, ".cbUserListFC_firstname a")
                    
                    lastname = lastname_link.text.strip()
                    firstname = firstname_link.text.strip()
                    profile_url = lastname_link.get_attribute("href")
                    
                    # Extraire l'adresse
                    try:
                        adresse_element = row.find_element(By.CSS_SELECTOR, ".cbUserListFC_cb_adresse1")
                        adresse = adresse_element.text.strip()
                    except NoSuchElementException:
                        adresse = ""
                    
                    # Extraire la ville
                    try:
                        ville_element = row.find_element(By.CSS_SELECTOR, ".cbUserListFC_cb_ville")
                        ville = ville_element.text.strip()
                    except NoSuchElementException:
                        ville = ""
                    
                    # Extraire le t√©l√©phone
                    try:
                        telephone_element = row.find_element(By.CSS_SELECTOR, ".cbUserListFC_cb_telephone")
                        telephone = telephone_element.text.strip()
                    except NoSuchElementException:
                        telephone = ""
                    
                    lawyer_info = {
                        'nom': lastname,
                        'prenom': firstname,
                        'nom_complet': f"{firstname} {lastname}",
                        'adresse': adresse,
                        'ville': ville,
                        'telephone': telephone,
                        'profile_url': profile_url
                    }
                    
                    lawyers_info.append(lawyer_info)
                
                except Exception as e:
                    logger.warning(f"Erreur lors de l'extraction de l'avocat {i+1} sur cette page: {e}")
                    continue
            
            return lawyers_info
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des avocats de cette page: {e}")
            return []
    
    def navigate_to_page(self, page_num):
        """Naviguer vers une page sp√©cifique"""
        try:
            if page_num == 1:
                self.driver.get(self.base_url)
                time.sleep(3)
                return True
            
            # Construire l'URL pour la page sp√©cifique
            # Format bas√© sur l'analyse de la pagination: limit=15&start=X
            start_value = (page_num - 1) * 15
            page_url = f"{self.base_url}userslist/Avocats.html?limit=15&start={start_value}"
            
            logger.info(f"Navigation vers la page {page_num}: {page_url}")
            self.driver.get(page_url)
            time.sleep(3)
            
            # V√©rifier que la page s'est bien charg√©e
            table = self.wait.until(EC.presence_of_element_located((By.ID, "cbUserTable")))
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de la navigation vers la page {page_num}: {e}")
            return False
    
    def extract_lawyer_details(self, lawyer_url):
        """Extraire les d√©tails complets depuis la fiche individuelle"""
        try:
            # Naviguer vers la page de l'avocat
            self.driver.get(lawyer_url)
            time.sleep(2)
            
            details = {}
            
            # R√©cup√©rer le code source pour l'analyse par regex
            page_source = self.driver.page_source
            
            # Chercher l'email
            email_patterns = [
                r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'mailto:([^"\'\\s]+)',
            ]
            
            for pattern in email_patterns:
                emails = re.findall(pattern, page_source, re.IGNORECASE)
                if emails:
                    for email in emails:
                        if '@' in email and '.' in email and len(email) > 5:
                            details['email'] = email.strip()
                            break
                    if 'email' in details:
                        break
            
            # Chercher l'ann√©e d'inscription au barreau
            inscription_patterns = [
                r'[Ii]nscrit(?:ion)?\s*.*?(\d{4})',
                r'[Bb]arreau\s*.*?(\d{4})',
                r'(\d{4})\s*[Ii]nscrit',
                r'depuis\s*(\d{4})',
                r'[Aa]dmis.*?(\d{4})',
            ]
            
            for pattern in inscription_patterns:
                matches = re.findall(pattern, page_source)
                if matches:
                    for year in matches:
                        if 1980 <= int(year) <= 2025:
                            details['annee_inscription'] = year
                            break
                    if 'annee_inscription' in details:
                        break
            
            # Chercher les sp√©cialisations - version am√©lior√©e
            try:
                body_text = self.driver.find_element(By.TAG_NAME, "body").text.lower()
                
                # Mots-cl√©s pour les domaines de sp√©cialisation
                specialization_keywords = [
                    'droit civil', 'droit p√©nal', 'droit commercial', 'droit du travail',
                    'droit de la famille', 'droit immobilier', 'droit des affaires',
                    'droit public', 'droit administratif', 'droit fiscal', 'droit social',
                    'droit international', 'droit europ√©en', 'droit de la sant√©',
                    'droit de l\'environnement', 'droit bancaire', 'propri√©t√© intellectuelle',
                    'droit des soci√©t√©s', 'droit du sport', 'droit de l\'urbanisme',
                    'droit rural', 'droit maritime', 'droit a√©rien', 'droit de l\'informatique'
                ]
                
                found_specializations = []
                for spec in specialization_keywords:
                    if spec in body_text:
                        found_specializations.append(spec.title())
                
                if found_specializations:
                    details['specialisations'] = '; '.join(found_specializations[:3])  # Max 3
                
            except Exception as e:
                logger.debug(f"Erreur extraction sp√©cialisations: {e}")
            
            # Chercher la structure
            structure_patterns = [
                r'[Cc]abinet\s+([A-Za-z\s&-]+)',
                r'SCP\s+([A-Za-z\s&-]+)',
                r'[Ss]oci√©t√©\s+([A-Za-z\s&-]+)',
                r'[Aa]ssociation\s+([A-Za-z\s&-]+)',
            ]
            
            for pattern in structure_patterns:
                matches = re.findall(pattern, page_source, re.IGNORECASE)
                if matches:
                    clean_structure = re.sub(r'<[^>]+>', '', matches[0]).strip()
                    if len(clean_structure) > 3 and len(clean_structure) < 100:
                        details['structure'] = clean_structure
                        break
            
            return details
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction des d√©tails: {e}")
            return {'erreur_extraction': str(e)}
    
    def run_complete_scraping(self):
        """Lancer le scraping complet de tous les avocats"""
        try:
            logger.info("=== D√âBUT DU SCRAPING COMPLET - CHALON-SUR-SA√îNE ===")
            start_time = datetime.now()
            
            # Aller sur la premi√®re page
            self.driver.get(self.base_url)
            time.sleep(5)
            
            # D√©tecter le nombre de pages
            total_pages = self.get_pagination_info()
            
            logger.info(f"üöÄ D√©but du scraping sur {total_pages} pages")
            
            # Parcourir toutes les pages
            for page_num in range(1, total_pages + 1):
                logger.info(f"\nüìÑ TRAITEMENT DE LA PAGE {page_num}/{total_pages}")
                
                # Naviguer vers la page
                if not self.navigate_to_page(page_num):
                    logger.error(f"Impossible d'acc√©der √† la page {page_num}")
                    continue
                
                # R√©cup√©rer les avocats de cette page
                page_lawyers = self.get_lawyers_from_current_page()
                
                if not page_lawyers:
                    logger.warning(f"Aucun avocat trouv√© sur la page {page_num}")
                    continue
                
                logger.info(f"‚úÖ {len(page_lawyers)} avocats trouv√©s sur la page {page_num}")
                
                # Pour chaque avocat, r√©cup√©rer les d√©tails
                for i, lawyer in enumerate(page_lawyers):
                    try:
                        logger.info(f"  üìã Traitement {i+1}/{len(page_lawyers)}: {lawyer['nom_complet']}")
                        
                        # Extraire les d√©tails depuis la fiche individuelle
                        details = self.extract_lawyer_details(lawyer['profile_url'])
                        
                        # Fusionner les informations
                        lawyer.update(details)
                        self.lawyers_data.append(lawyer)
                        
                        # Pause courte entre les requ√™tes
                        time.sleep(1)
                        
                    except Exception as e:
                        logger.error(f"  ‚ùå Erreur lors du traitement de {lawyer.get('nom_complet', 'Avocat inconnu')}: {e}")
                        continue
                
                # Sauvegarde interm√©diaire tous les 2 pages
                if page_num % 2 == 0:
                    self.save_intermediate_results(page_num)
                
                logger.info(f"üìä Page {page_num} termin√©e. Total trait√©: {len(self.lawyers_data)} avocats")
            
            # Sauvegarde finale
            self.save_final_results()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info(f"\nüéâ SCRAPING TERMIN√â!")
            logger.info(f"‚è±Ô∏è  Dur√©e totale: {duration}")
            logger.info(f"üë• Total d'avocats r√©cup√©r√©s: {len(self.lawyers_data)}")
            
            # Statistiques finales
            self.print_final_statistics()
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©rale dans le scraping complet: {e}")
        
        finally:
            if hasattr(self, 'driver'):
                self.driver.quit()
    
    def save_intermediate_results(self, page_num):
        """Sauvegarder les r√©sultats interm√©diaires"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        filename = f"chalon_partial_p{page_num}_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.lawyers_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ Sauvegarde interm√©diaire: {filename}")
    
    def save_final_results(self):
        """Sauvegarder les r√©sultats finaux"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV complet
        csv_filename = f"chalon_COMPLET_{timestamp}.csv"
        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
            if self.lawyers_data:
                fieldnames = ['nom', 'prenom', 'nom_complet', 'email', 'telephone', 
                            'adresse', 'ville', 'annee_inscription', 'specialisations', 
                            'structure', 'profile_url']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
                writer.writeheader()
                writer.writerows(self.lawyers_data)
        
        # JSON complet
        json_filename = f"chalon_COMPLET_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as jsonfile:
            json.dump(self.lawyers_data, jsonfile, ensure_ascii=False, indent=2)
        
        # Emails seulement
        emails_filename = f"chalon_EMAILS_SEULEMENT_{timestamp}.txt"
        with open(emails_filename, 'w', encoding='utf-8') as emailfile:
            emails = [lawyer.get('email', '') for lawyer in self.lawyers_data if lawyer.get('email')]
            for email in sorted(set(emails)):
                emailfile.write(f"{email}\n")
        
        # Rapport final
        report_filename = f"chalon_RAPPORT_COMPLET_{timestamp}.txt"
        with open(report_filename, 'w', encoding='utf-8') as reportfile:
            reportfile.write("=== RAPPORT COMPLET - BARREAU CHALON-SUR-SA√îNE ===\n\n")
            reportfile.write(f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            reportfile.write(f"Nombre total d'avocats: {len(self.lawyers_data)}\n\n")
            
            # Statistiques
            emails_count = sum(1 for l in self.lawyers_data if l.get('email'))
            phones_count = sum(1 for l in self.lawyers_data if l.get('telephone'))
            years_count = sum(1 for l in self.lawyers_data if l.get('annee_inscription'))
            specs_count = sum(1 for l in self.lawyers_data if l.get('specialisations'))
            
            reportfile.write(f"STATISTIQUES:\n")
            reportfile.write(f"- Emails trouv√©s: {emails_count}/{len(self.lawyers_data)} ({emails_count/len(self.lawyers_data)*100:.1f}%)\n")
            reportfile.write(f"- T√©l√©phones: {phones_count}/{len(self.lawyers_data)} ({phones_count/len(self.lawyers_data)*100:.1f}%)\n")
            reportfile.write(f"- Ann√©es inscription: {years_count}/{len(self.lawyers_data)} ({years_count/len(self.lawyers_data)*100:.1f}%)\n")
            reportfile.write(f"- Sp√©cialisations: {specs_count}/{len(self.lawyers_data)} ({specs_count/len(self.lawyers_data)*100:.1f}%)\n\n")
        
        logger.info(f"‚úÖ R√âSULTATS FINAUX SAUVEGARD√âS:")
        logger.info(f"üìÑ CSV complet: {csv_filename}")
        logger.info(f"üìÑ JSON complet: {json_filename}")
        logger.info(f"üìß Emails uniquement: {emails_filename}")
        logger.info(f"üìä Rapport: {report_filename}")
    
    def print_final_statistics(self):
        """Afficher les statistiques finales"""
        if not self.lawyers_data:
            return
        
        emails_found = sum(1 for lawyer in self.lawyers_data if lawyer.get('email'))
        phones_found = sum(1 for lawyer in self.lawyers_data if lawyer.get('telephone'))
        years_found = sum(1 for lawyer in self.lawyers_data if lawyer.get('annee_inscription'))
        specs_found = sum(1 for lawyer in self.lawyers_data if lawyer.get('specialisations'))
        structures_found = sum(1 for lawyer in self.lawyers_data if lawyer.get('structure'))
        
        total = len(self.lawyers_data)
        
        logger.info(f"\nüìä STATISTIQUES FINALES:")
        logger.info(f"üë• Total d'avocats: {total}")
        logger.info(f"üìß Emails: {emails_found}/{total} ({emails_found/total*100:.1f}%)")
        logger.info(f"üìû T√©l√©phones: {phones_found}/{total} ({phones_found/total*100:.1f}%)")
        logger.info(f"üìÖ Ann√©es inscription: {years_found}/{total} ({years_found/total*100:.1f}%)")
        logger.info(f"‚öñÔ∏è  Sp√©cialisations: {specs_found}/{total} ({specs_found/total*100:.1f}%)")
        logger.info(f"üè¢ Structures: {structures_found}/{total} ({structures_found/total*100:.1f}%)")

def main():
    """Fonction principale avec choix du mode"""
    import sys
    
    print("üèõÔ∏è  SCRAPER BARREAU DE CHALON-SUR-SA√îNE üèõÔ∏è")
    print("=" * 50)
    
    if len(sys.argv) > 1 and sys.argv[1] == '--visual':
        print("Mode: VISUEL (pour debug)")
        headless = False
    else:
        print("Mode: HEADLESS (production)")
        headless = True
    
    print(f"URL: https://www.avocats-chalonsursaone.com/")
    print("=" * 50)
    
    scraper = ChalonsurSaoneBarScraperProduction(headless=headless)
    scraper.run_complete_scraping()

if __name__ == "__main__":
    main()