#!/usr/bin/env python3
"""
SCRAPER PRODUCTION - BARREAU DE GRASSE
Script final pour extraire TOUS les avocats en mode headless (sans interface)
Toutes les donn√©es : pr√©nom, nom, email, t√©l√©phone, sp√©cialisations, adresse, etc.
"""

import time
import json
import csv
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import re

class GrasseProductionScraper:
    def __init__(self):
        self.setup_driver()
        self.lawyers_data = []
        self.base_url = "https://www.avocats-grasse.com/fr/annuaire-avocats"
        
    def setup_driver(self):
        """Configuration du driver Chrome en mode headless"""
        chrome_options = Options()
        
        # Mode headless et optimisations
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-logging")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 15)
        
        print("ü§ñ Driver Chrome configur√© en mode headless")
        
    def accept_cookies_if_present(self):
        """Accepter les cookies si la banni√®re est pr√©sente"""
        try:
            cookie_selectors = [
                "button[id*='cookie']",
                "button[class*='cookie']", 
                "button[id*='accept']",
                "button[class*='accept']",
                ".cookie-consent button",
                "#cookie-consent button",
                ".gdpr-consent button"
            ]
            
            for selector in cookie_selectors:
                try:
                    cookie_button = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
                    cookie_button.click()
                    print(f"‚úÖ Cookies accept√©s")
                    time.sleep(2)
                    return True
                except TimeoutException:
                    continue
                    
            return False
            
        except Exception as e:
            return False
    
    def extract_lawyer_info(self, lawyer_element):
        """Extraire les informations compl√®tes d'un avocat"""
        lawyer_data = {
            'prenom': '',
            'nom': '',
            'email': '',
            'telephone': '',
            'adresse': '',
            'code_postal': '',
            'ville': '',
            'specialisations': [],
            'annee_inscription': '',
            'structure': '',
            'site_web': ''
        }
        
        try:
            # R√©cup√©rer tout le texte de l'√©l√©ment
            full_text = lawyer_element.text.strip()
            
            # 1. Nom et pr√©nom - Format: "NOM Pr√©nom Ville (Code postal)"
            try:
                name_element = lawyer_element.find_element(By.CSS_SELECTOR, "h3, h4, .name, strong")
                name_text = name_element.text.strip()
                
                # Parser avec regex : "NOM Pr√©nom Ville (Code postal)"
                name_match = re.match(r'([A-Z\s-]+?)\s+([A-Za-z√Ä-√ø\s-]+?)\s+([A-Za-z√Ä-√ø\s-]+)\s*\((\d{5})\)', name_text)
                
                if name_match:
                    lawyer_data['nom'] = name_match.group(1).strip()
                    lawyer_data['prenom'] = name_match.group(2).strip()
                    lawyer_data['ville'] = name_match.group(3).strip()
                    lawyer_data['code_postal'] = name_match.group(4).strip()
                else:
                    # Format alternatif: essayer juste "NOM Pr√©nom"
                    name_parts = name_text.split()
                    if len(name_parts) >= 2:
                        lawyer_data['nom'] = name_parts[0]
                        lawyer_data['prenom'] = " ".join(name_parts[1:])
                        
            except NoSuchElementException:
                # Extraire depuis la premi√®re ligne du texte
                first_line = full_text.split('\n')[0] if full_text else ""
                name_match = re.match(r'([A-Z\s-]+?)\s+([A-Za-z√Ä-√ø\s-]+)', first_line)
                if name_match:
                    lawyer_data['nom'] = name_match.group(1).strip()
                    lawyer_data['prenom'] = name_match.group(2).strip()
            
            # 2. Email
            try:
                email_element = lawyer_element.find_element(By.CSS_SELECTOR, "a[href^='mailto:']")
                lawyer_data['email'] = email_element.get_attribute('href').replace('mailto:', '')
            except NoSuchElementException:
                # Recherche avec regex dans le texte
                email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', full_text)
                if email_match:
                    lawyer_data['email'] = email_match.group(1)
            
            # 3. T√©l√©phone
            try:
                phone_element = lawyer_element.find_element(By.CSS_SELECTOR, "a[href^='tel:']")
                lawyer_data['telephone'] = phone_element.text.strip()
            except NoSuchElementException:
                # Recherche avec regex
                phone_matches = re.findall(r'(?:T√©l\s*[:.]?\s*)?(\d{2}[\s.-]?\d{2}[\s.-]?\d{2}[\s.-]?\d{2}[\s.-]?\d{2})', full_text)
                if phone_matches:
                    lawyer_data['telephone'] = phone_matches[0]
            
            # 4. Sp√©cialisations depuis "Domaines d'activit√©s"
            specializations_match = re.search(r'Domaines d\'activit√©s\s*[:\n]\s*(.+?)(?:\n|$)', full_text, re.IGNORECASE | re.MULTILINE)
            if specializations_match:
                spec_text = specializations_match.group(1).strip()
                # Nettoyer le texte
                spec_text = re.sub(r'^[:\s,]+', '', spec_text)
                spec_text = re.sub(r'[,\s]+$', '', spec_text)
                
                if spec_text:
                    # S√©parer par les virgules
                    specializations = [s.strip() for s in spec_text.split(',') if s.strip()]
                    lawyer_data['specialisations'] = specializations
            
            # 5. Adresse depuis Google Maps
            try:
                maps_link = lawyer_element.find_element(By.CSS_SELECTOR, "a[href*='maps.google']")
                maps_url = maps_link.get_attribute('href')
                
                # Extraire l'adresse depuis l'URL
                address_match = re.search(r'q=([^&]+)', maps_url)
                if address_match:
                    from urllib.parse import unquote
                    address_encoded = address_match.group(1)
                    address_decoded = unquote(address_encoded).replace('%0D%0A', '\n')
                    
                    # Parser l'adresse
                    address_lines = address_decoded.split(',')
                    if len(address_lines) >= 1:
                        # Premi√®re partie = adresse
                        lawyer_data['adresse'] = address_lines[0].strip()
                        
                        # Rechercher code postal et ville s'ils ne sont pas d√©j√† remplis
                        if not lawyer_data['code_postal'] or not lawyer_data['ville']:
                            for part in address_lines:
                                postal_match = re.search(r'(\d{5})', part)
                                if postal_match and not lawyer_data['code_postal']:
                                    lawyer_data['code_postal'] = postal_match.group(1)
                                
                                # Ville = partie avec lettres mais sans code postal
                                if re.search(r'[A-Za-z√Ä-√ø\s-]+', part) and not re.search(r'\d{5}', part) and not lawyer_data['ville']:
                                    city_clean = re.sub(r'^\s*,?\s*', '', part).strip()
                                    if city_clean and len(city_clean) > 1:
                                        lawyer_data['ville'] = city_clean
                        
            except NoSuchElementException:
                pass
            
            # 6. Ann√©e d'inscription (si mentionn√©e)
            year_match = re.search(r'(?:inscrit|inscription).*?(\d{4})', full_text, re.IGNORECASE)
            if year_match:
                year = int(year_match.group(1))
                if 1950 <= year <= datetime.now().year:  # Validation de l'ann√©e
                    lawyer_data['annee_inscription'] = str(year)
            
            # 7. Structure/Cabinet
            cabinet_patterns = [
                r'Cabinet\s+([A-Za-z√Ä-√ø\s&.-]+?)(?:\n|$)',
                r'SCP\s+([A-Za-z√Ä-√ø\s&.-]+?)(?:\n|$)',
                r'Soci√©t√©\s+([A-Za-z√Ä-√ø\s&.-]+?)(?:\n|$)',
                r'SELARL\s+([A-Za-z√Ä-√ø\s&.-]+?)(?:\n|$)',
                r'([A-Za-z√Ä-√ø\s&.-]+)\s+[Aa]vocats?(?:\n|$)'
            ]
            
            for pattern in cabinet_patterns:
                cabinet_match = re.search(pattern, full_text, re.IGNORECASE | re.MULTILINE)
                if cabinet_match:
                    structure = cabinet_match.group(1).strip()
                    # Validation : ne pas prendre des noms de personnes
                    if len(structure) > 3 and not re.match(r'^[A-Z]+\s+[A-Za-z]+$', structure):
                        lawyer_data['structure'] = structure
                        break
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur extraction avocat: {e}")
        
        return lawyer_data
    
    def get_lawyers_from_page(self, page_num):
        """Extraire tous les avocats de la page courante"""
        lawyers = []
        
        try:
            # Attendre le chargement complet de la page
            time.sleep(4)
            
            # Les avocats sont dans des balises <article>
            lawyer_elements = self.driver.find_elements(By.CSS_SELECTOR, "article")
            
            if not lawyer_elements:
                print(f"‚ö†Ô∏è  Aucun √©l√©ment <article> trouv√© sur la page {page_num}")
                return lawyers
                
            print(f"üìã {len(lawyer_elements)} avocats d√©tect√©s sur la page {page_num}")
            
            for i, element in enumerate(lawyer_elements):
                try:
                    lawyer_data = self.extract_lawyer_info(element)
                    
                    # Validation : doit avoir au moins un nom ou un email
                    if lawyer_data['nom'] or lawyer_data['prenom'] or lawyer_data['email']:
                        lawyers.append(lawyer_data)
                    else:
                        print(f"   ‚ö†Ô∏è  Avocat {i+1} ignor√© (donn√©es insuffisantes)")
                    
                except Exception as e:
                    print(f"   ‚ùå Erreur avocat {i+1}: {e}")
                    continue
                    
            print(f"‚úÖ {len(lawyers)} avocats valides extraits de la page {page_num}")
                    
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction des avocats page {page_num}: {e}")
            
        return lawyers
    
    def get_total_pages(self):
        """D√©tecter le nombre total de pages"""
        try:
            # Chercher les liens de pagination
            page_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='page-']")
            max_page = 1
            
            for link in page_links:
                href = link.get_attribute('href') or ''
                text = link.text.strip()
                
                # Extraire depuis l'href
                page_match = re.search(r'page-(\d+)', href)
                if page_match:
                    page_num = int(page_match.group(1))
                    max_page = max(max_page, page_num)
                
                # Ou depuis le texte
                if text.isdigit():
                    page_num = int(text)
                    max_page = max(max_page, page_num)
                    
            return max_page
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur d√©tection pages: {e}")
            return 1
    
    def navigate_to_page(self, page_num):
        """Naviguer vers une page sp√©cifique"""
        try:
            url = f"{self.base_url}/page-{page_num}"
            self.driver.get(url)
            
            # Attendre que la page se charge
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article")))
            time.sleep(2)
            
            return True
        except Exception as e:
            print(f"‚ùå Erreur navigation page {page_num}: {e}")
            return False
    
    def scrape_all_pages(self):
        """Scraper TOUTES les pages de l'annuaire"""
        print("üöÄ D√âBUT DU SCRAPING COMPLET - MODE PRODUCTION")
        print("=" * 70)
        
        try:
            # Page 1
            print("üîó Connexion √† la premi√®re page...")
            self.driver.get(f"{self.base_url}/page-1")
            
            # Accepter les cookies
            self.accept_cookies_if_present()
            
            # D√©tecter le nombre total de pages
            total_pages = self.get_total_pages()
            print(f"üìÑ TOTAL DE PAGES D√âTECT√âES: {total_pages}")
            print(f"üéØ Scraping pr√©vu sur {total_pages} pages\n")
            
            start_time = time.time()
            
            for page_num in range(1, total_pages + 1):
                page_start = time.time()
                print(f"üìñ PAGE {page_num}/{total_pages}")
                
                # Navigation (sauf pour la premi√®re page)
                if page_num > 1:
                    success = self.navigate_to_page(page_num)
                    if not success:
                        print(f"   ‚è≠Ô∏è  Page {page_num} ignor√©e (erreur navigation)")
                        continue
                
                # Extraction des avocats
                page_lawyers = self.get_lawyers_from_page(page_num)
                
                if page_lawyers:
                    self.lawyers_data.extend(page_lawyers)
                    print(f"   ‚úÖ {len(page_lawyers)} avocats ajout√©s")
                else:
                    print(f"   ‚ö†Ô∏è  Aucun avocat extrait de cette page")
                
                page_time = time.time() - page_start
                total_so_far = len(self.lawyers_data)
                print(f"   ‚è±Ô∏è  Page trait√©e en {page_time:.1f}s - Total: {total_so_far} avocats")
                
                # Sauvegarde interm√©diaire tous les 5 pages
                if page_num % 5 == 0:
                    print(f"   üíæ Sauvegarde interm√©diaire...")
                    self.save_results(f"grasse_partial_p{page_num}")
                
                print()  # Ligne vide pour la lisibilit√©
                
            # Statistiques finales
            total_time = time.time() - start_time
            print("üéâ SCRAPING COMPLET TERMIN√â!")
            print("=" * 50)
            print(f"üìä TOTAL D'AVOCATS EXTRAITS: {len(self.lawyers_data)}")
            print(f"‚è±Ô∏è  TEMPS TOTAL: {total_time/60:.1f} minutes")
            print(f"üìà MOYENNE: {len(self.lawyers_data)/total_time*60:.1f} avocats/minute")
            
            return self.lawyers_data
            
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  SCRAPING INTERROMPU PAR L'UTILISATEUR")
            if self.lawyers_data:
                print(f"üíæ Sauvegarde des {len(self.lawyers_data)} avocats d√©j√† extraits...")
                self.save_results("grasse_interrupted")
            return self.lawyers_data
            
        except Exception as e:
            print(f"\n‚ùå ERREUR CRITIQUE: {e}")
            if self.lawyers_data:
                print(f"üíæ Sauvegarde de secours ({len(self.lawyers_data)} avocats)...")
                self.save_results("grasse_emergency")
            return []
    
    def save_results(self, filename_prefix="grasse_production"):
        """Sauvegarder tous les r√©sultats avec rapports d√©taill√©s"""
        if not self.lawyers_data:
            print("‚ùå Aucune donn√©e √† sauvegarder")
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        total_lawyers = len(self.lawyers_data)
        
        print(f"üíæ Sauvegarde de {total_lawyers} avocats...")
        
        # 1. JSON complet
        json_filename = f"{filename_prefix}_{total_lawyers}_avocats_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.lawyers_data, f, indent=2, ensure_ascii=False)
        print(f"   ‚úÖ JSON: {json_filename}")
        
        # 2. CSV complet
        csv_filename = f"{filename_prefix}_{total_lawyers}_avocats_{timestamp}.csv"
        with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
            if self.lawyers_data:
                fieldnames = ['prenom', 'nom', 'email', 'telephone', 'adresse', 'code_postal', 'ville', 
                            'specialisations', 'annee_inscription', 'structure', 'site_web']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for lawyer in self.lawyers_data:
                    lawyer_copy = lawyer.copy()
                    lawyer_copy['specialisations'] = '; '.join(lawyer['specialisations']) if lawyer['specialisations'] else ''
                    writer.writerow(lawyer_copy)
        print(f"   ‚úÖ CSV: {csv_filename}")
        
        # 3. Fichier emails uniquement
        emails_filename = f"{filename_prefix}_EMAILS_SEULEMENT_{timestamp}.txt"
        with open(emails_filename, 'w', encoding='utf-8') as f:
            emails_count = 0
            for lawyer in self.lawyers_data:
                if lawyer['email']:
                    f.write(f"{lawyer['email']}\n")
                    emails_count += 1
        print(f"   üìß {emails_count} Emails: {emails_filename}")
        
        # 4. Rapport de production d√©taill√©
        report_filename = f"{filename_prefix}_RAPPORT_COMPLET_{timestamp}.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"RAPPORT DE PRODUCTION - SCRAPING BARREAU DE GRASSE\n")
            f.write(f"{'='*70}\n\n")
            f.write(f"Date de production: {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}\n")
            f.write(f"URL source: {self.base_url}\n")
            f.write(f"Nombre total d'avocats extraits: {total_lawyers}\n\n")
            
            # Statistiques de qualit√©
            with_email = sum(1 for l in self.lawyers_data if l['email'])
            with_phone = sum(1 for l in self.lawyers_data if l['telephone'])
            with_specializations = sum(1 for l in self.lawyers_data if l['specialisations'])
            with_address = sum(1 for l in self.lawyers_data if l['adresse'])
            with_structure = sum(1 for l in self.lawyers_data if l['structure'])
            
            f.write("STATISTIQUES DE QUALIT√â DES DONN√âES:\n")
            f.write(f"- Avocats avec email: {with_email} ({with_email/total_lawyers*100:.1f}%)\n")
            f.write(f"- Avocats avec t√©l√©phone: {with_phone} ({with_phone/total_lawyers*100:.1f}%)\n")
            f.write(f"- Avocats avec sp√©cialisations: {with_specializations} ({with_specializations/total_lawyers*100:.1f}%)\n")
            f.write(f"- Avocats avec adresse compl√®te: {with_address} ({with_address/total_lawyers*100:.1f}%)\n")
            f.write(f"- Avocats avec structure/cabinet: {with_structure} ({with_structure/total_lawyers*100:.1f}%)\n\n")
            
            # R√©partition g√©ographique
            cities = {}
            for lawyer in self.lawyers_data:
                city = lawyer['ville']
                if city:
                    cities[city] = cities.get(city, 0) + 1
            
            if cities:
                f.write("R√âPARTITION G√âOGRAPHIQUE (TOP 15):\n")
                for city, count in sorted(cities.items(), key=lambda x: x[1], reverse=True)[:15]:
                    percentage = count/total_lawyers*100
                    f.write(f"- {city}: {count} avocats ({percentage:.1f}%)\n")
                f.write(f"\nTotal villes repr√©sent√©es: {len(cities)}\n\n")
            
            # Sp√©cialisations les plus courantes
            all_specializations = []
            for lawyer in self.lawyers_data:
                all_specializations.extend(lawyer['specialisations'])
            
            if all_specializations:
                spec_counts = {}
                for spec in all_specializations:
                    spec_counts[spec] = spec_counts.get(spec, 0) + 1
                
                f.write("SP√âCIALISATIONS LES PLUS REPR√âSENT√âES (TOP 20):\n")
                for spec, count in sorted(spec_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
                    percentage = count/total_lawyers*100
                    f.write(f"- {spec}: {count} mentions ({percentage:.1f}%)\n")
                f.write(f"\nTotal sp√©cialisations diff√©rentes: {len(spec_counts)}\n\n")
            
            # Exemples d'extraction
            f.write("EXEMPLES D'AVOCATS EXTRAITS:\n")
            f.write("-" * 40 + "\n")
            for i, lawyer in enumerate(self.lawyers_data[:10]):
                f.write(f"{i+1}. {lawyer['prenom']} {lawyer['nom']}\n")
                f.write(f"   üìß Email: {lawyer['email'] or 'Non renseign√©'}\n")
                f.write(f"   üìû T√©l√©phone: {lawyer['telephone'] or 'Non renseign√©'}\n")
                f.write(f"   üìç Adresse: {lawyer['adresse']} - {lawyer['ville']} ({lawyer['code_postal']})\n")
                f.write(f"   üè¢ Structure: {lawyer['structure'] or 'Non renseign√©e'}\n")
                f.write(f"   ‚öñÔ∏è  Sp√©cialisations: {', '.join(lawyer['specialisations']) or 'Non renseign√©es'}\n\n")
            
            # Instructions d'utilisation
            f.write("FICHIERS G√âN√âR√âS:\n")
            f.write(f"- Donn√©es compl√®tes JSON: {json_filename}\n")
            f.write(f"- Donn√©es compl√®tes CSV: {csv_filename}\n")  
            f.write(f"- Liste emails seuls: {emails_filename}\n")
            f.write(f"- Ce rapport: {report_filename}\n\n")
            
            f.write("UTILISATION RECOMMAND√âE:\n")
            f.write("- Import Excel/Google Sheets: utiliser le fichier CSV\n")
            f.write("- Mailing list: utiliser le fichier emails TXT\n")
            f.write("- D√©veloppement/API: utiliser le fichier JSON\n")
            f.write("- Analyse: consulter ce rapport\n")
        
        print(f"   üìã Rapport: {report_filename}")
        print(f"\nüéâ SAUVEGARDE TERMIN√âE - {total_lawyers} avocats sauvegard√©s!")
    
    def close(self):
        """Fermer le navigateur"""
        if self.driver:
            self.driver.quit()

def main():
    print("üèõÔ∏è  SCRAPER PRODUCTION - BARREAU DE GRASSE")
    print("=" * 70)
    print("‚ö° Mode headless activ√© (pas d'interface visuelle)")
    print("üìä Extraction de TOUS les avocats avec toutes les donn√©es")
    print("üéØ Donn√©es extraites: nom, pr√©nom, email, t√©l, sp√©cialisations, adresse...")
    print()
    
    scraper = None
    
    try:
        scraper = GrasseProductionScraper()
        
        # Lancer le scraping complet
        results = scraper.scrape_all_pages()
        
        if results:
            # Sauvegarde finale
            scraper.save_results("GRASSE_PRODUCTION_FINALE")
            
            print("\n" + "="*70)
            print("‚úÖ SUCC√àS COMPLET!")
            print(f"üìä {len(results)} avocats du barreau de Grasse extraits")
            print("üìÅ Tous les fichiers sont sauvegard√©s dans le r√©pertoire courant")
            print("="*70)
        else:
            print("\n‚ùå √âCHEC - Aucune donn√©e extraite")
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Script interrompu par l'utilisateur")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        
    finally:
        if scraper:
            scraper.close()
            print("üîí Navigateur ferm√©")

if __name__ == "__main__":
    main()